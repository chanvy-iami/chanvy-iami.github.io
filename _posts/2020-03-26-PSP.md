---
layout: post
title: Python3爬虫实战
subtitle: 基本库的使用
date: 2020-03-26
author: Weiami
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - Python Spider
---

## 使用urllib

urllib库是Python内置的HTTP请求库，无需安装。

* request：HTTP请求模块，用来模拟发送请求
* error：异常处理模块，请求错误会进行重试或其他操作
* parse：工具模块，URL的处理方法
* robotparser：识别网站的robot.txt文件

### 发送请求

1.urlopen()

```
import urllib.request
response = urllib.request.urlopen('https://www.python.org')
print(type(response))
print(response.read().decode('utf-8'))
```

其返回是一个HTTPResponse类型对象，包含read()、readinto()、getheader(name)、getheaders()和fileno()等方法；msg、version、status、reason、debuglevel和closed等属性。

利用urlopen()方法，可完成最基本的网页GET()请求抓取。

`urllib.request.urlopen(url, data=None, [timeout,]*, cafile=None, capath=None, cadefault=False, context=None)`

* data参数：可选，必须是bytes类型，传递此参数后，不再是GET方式，而是POST方式

```
import urllib.parse
import urllib.request
data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())
```

* timeout参数：设置超时时间，单位秒，请求超出此时间，抛出异常

```
import urllib.request
response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)
print(response.read())
```

* 其他参数：cafile和capath为CA证书及其路径，cadefault已弃用，context参数必须是ssl.SSLContext，指定SSL设置。

2.Request

如果请求中需要加入Headers等信息，则需Request类来构建请求。

```
import urllib.request
request = urllib.request.Request('http://python.org')
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

`class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`

* url为必传参数
* data参数
* headers为一个字典，请求头，可直接构造，也可通过add_header()方法添加
* origin_req_host是请求方的host名称或者IP地址
* unverifiable表示这个请求是否无法验证，默认False
* method为一个字符串，指示请求使用的方法

3.高级用法

url.request模块里的BaseHandler类，是其他Handler的父类。

* HTTPDefaultErrorHandler：用于HTTP响应错误，可抛出HTTPError类型异常
* HTTPRedirectHandler：用于处理重定向
* HTTPCookieProcessor：用于处理Cookies
* ProxyHandler：用于设置代理，默认代理为空
* HTTPPasswordMgr：用于管理密码，维护用户名和密码的表
* HTTPBasicAuthHandler：用于管理认证

另一个重要的类是OpenerDirector,称为Opener。

* 验证
* 代理
* Cookies

具体实现过程参考《Python3网络爬虫开发实战》。

### 处理异常

1.URLError：来自urllib库的error模块的类。

```
from urllib import request, error
try:
    response = request.urlopen('http://baidu.com/index.htm')
except error.URLError as e:
    print(e.reason)
```

程序不会直接报错，而是避免程序异常终止。

2.HTTPError:URLError的子类，专门处理HTTP请求错误，有3个属性。

* code:返回HTTP状态码
* reason:返回错误原因
* headers:返回请求头

```
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
```

因为URLError是HTTPError的父类，所以可以先捕获子类的错误，然后在捕获父类的错误。

### 解析链接

1.urlparse():实现URL的识别和分段。

```
from urllib.parse import urlparse
result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result), result)
```

返回ParseResult类型对象，包括6个部分，scheme、netloc、path、params、query和fragment。协议、域名、访问路径、分号后为参数，问号后为查询条件，井号后是锚点。

API用法：

`urllib.parse.urlparse(urlstring, scheme='', allow_fragment=True)`

* urlstring:待解析的URL
* scheme:默认协议
* allow_fragments:是否忽略fragment

```
from urllib.parse import urlparse
result1 = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')
print(result1)
result2 = urlparse('http://www.baidu.com/index.html;user?id=5#comment', scheme='https')
print(result2)
```

scheme参数只有在URL中不包含scheme信息时才生效。

2.urlunparse():接受参数为可迭代对象，长度必须是6。实现URL的构造。

```
from urllib.parse import urlunparse
data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
```

3.urlsplit():与urlparse()相似，不单独解析params这一部分，返回5个结果。

```
from urllib.parse import urlsplit
result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(result) 
```

返回一个元组类型的SplitResult，既可用属性获取值，也可以用索引获取。

`print(result.scheme, result[0])`

4.urlunsplit()

```
from urllib.parse import urlunsplit
data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
print(urlunsplit(data))
```

5.urljoin()

提供base_url(基础链接)作为第一个参数，新的链接为第二个参数，经过分析基础链接的内容，将新链接缺失部分补充，并返回结果。

```
from urllib.parse import urljoin
print(urljoin('http://www.baidu.com', 'FAQ.html'))
```

6.urlencode()

```
from urllib.parse import urlencode
params = {'name': 'germey', 'age': 22}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```

首先声明字典来表示参数，然后调用urlencode()方法将其序列化为GET请求参数。

7.parse_qs():反序列化。

```
from urllib.parse import parse_qs
query = 'name=germey&age=22'
print(parse_qs(query))
```

8.parse_qsl():将参数转化为元组组成的列表。

```
from urllib.parse import parse_qsl
query = 'name=germey&age=22'
print(parse_qsl(query))
```

9.quote():将内容转化为URL编码的格式。

```
from urllib.parse import quote
keyword = '壁纸'
url = 'http://www.baidu.com/s?wd=' + quote(keyword)
print(url)
```

10.unquote():对URL解码。

```
from urllib.parse import unquote
url = 'http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
print(unquote(url))
```

### 分析Robots协议

1.Robots协议

Robots协议也称作爬虫协议，用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取，是一个robots.txt文本文件。

```
User-agent: *
Disallow: /
Allow: /public/
```

* User-agent描述搜索爬虫的名称，*代表该协议对任何爬虫有效
* Disallow指定不允许抓取的目录，/代表不允许抓取所有页面
* Allow表示所有页面不允许抓取，但可以抓取public目录

2.爬虫名称

* 百度的爬虫名称：BaiduSpider
* 谷歌的爬虫名称：Googlebot
* 更多请参考《Python3网络爬虫开发实战》

3.robotparser

使用robotparser模块来解析robots.txt，并提供一个类RobotFileParser。类的声明如下：

`urllib.robotparser.RobotFileParser(url='')`

* set_url():设置robots.txt文件的链接
* read():读取robots.txt文件并进行分析
* parse():解析robots.txt文件
* can_fetch():传入两个参数，第一个为User-agent，第二个为URL。判断是否可以抓取这个URL，返回结果是True或False。
* mtime():返回上次抓取和分析robots.txt的时间
* modified():将当前时间设置为上次抓取和分析robots.txt的时间

```
from urllib.robotparser import RobotFileParser
rp = RobotFileParser()
rp.set_url('http://www.jianshu.com/robots.txt')
rp.read()
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*', 'http://www.jianshu.com/search?q=python&page=1&type=collections))
```

